{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Material adapted from:\n",
    "MOOC \"Scalable Machine Learning\", EdX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EVALUATION FUNCTION AND BASELINE MODEL\n",
    "\n",
    "This notebook has two main goals: choose a baseline model and choose an evaluation function.\n",
    "\n",
    "The process of selecting model and function are explained throughout the notebook. To sum it up:\n",
    "\n",
    "* **Evaluation function** = LogLoss (lower values are better)\n",
    "\n",
    "$$  \\begin{align} \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\end{align} $$ \n",
    "\n",
    "* **Baseline model chosen** = always predict value 0.06998 (average) instead of most frequent (0): \n",
    "\n",
    "    Baseline Logloss (predicted = 0, calculated by function) = 0.17725\n",
    "    Baseline Logloss (predicted = 0.06998, calculated by function) = 0.04170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###IMPORT PACKAGES\n",
    "\n",
    "Let's import packages used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#VISUALIZATION\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###IMPORT DATA AND CHOOSE A BASELINE MODEL.\n",
    "\n",
    "*\"A very simple yet natural baseline model is one where we always make the same prediction independent of the given datapoint, setting the predicted value equal to the fraction of training points that correspond to click-through events (i.e., where the label is one). Compute this value (which is simply the mean of the training labels), and then use it to compute the training log loss for the baseline model. The log loss for multiple observations is the mean of the individual log loss values.Â¶\" *(source: \"Scalable Machine Learning\", EdX)\n",
    "\n",
    "So in order to calculate the predicted value for the baseline model, we will use the known outcome (stored at *\"outcome.csv\"*)\n",
    "\n",
    "So first, we import the file \"outcome.csv\", stored inside a folder named \"DATA\":\n",
    "\n",
    "*\\DATA\\outcome.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0.1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '0.1'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use panda to import csv\n",
    "df = pd.read_csv('DATA/outcome.csv', sep=';')\n",
    "print df.columns.values\n",
    "\n",
    "#Column 0 includes ad_ref. Column 1 refers to outcome (0= no click, 1=click)\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: 0.1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df[\"0.1\"][1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe \"df\" has two columns. The second one \"0.1\" contains the outcome values (1 = clicked, 0 = not-clicked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(681313L,)\n"
     ]
    }
   ],
   "source": [
    "print df[\"0.1\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 681313 cases. \n",
    "\n",
    "#####Which predicted value to use?\n",
    "\n",
    "We have two options: to use the most frequent value or to use the average value.\n",
    "\n",
    "####A. Baseline predicted value = most frequent \n",
    "Let's check the label values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEZCAYAAACjPJNSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA8fMzLJw4JiZWRb/H0BBtMp0ZfoUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ed3bcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's plot the outcome values to see wich value is more common, 0 (no-click) or 1 (click).\n",
    "\n",
    "plt.hist(df[\"0.1\"])\n",
    "plt.title(\"Click-through-rate\")\n",
    "plt.xlabel(\"Outcome\")\n",
    "plt.ylabel(\"#clicks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Most of values are 0. Baseline will be to predict outcome=0\n",
    "#Let's add a column to df called \"predict\" that will have the predicted value (0)\n",
    "df[\"predict1\"]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this model performs. First the confussion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[676545,      0],\n",
       "       [  4768,      0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EVALUATE MODEL\n",
    "#Let's see the confussion matrix:\n",
    "\n",
    "y_pred = df[\"predict1\"]\n",
    "y_true = df[\"0.1\"]\n",
    "\n",
    "'''#print model score\n",
    "print model.score(y_pred,y_true)'''\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate performance metrics (accuracy, sensitivity, specificity...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE MODEL SCORE\n",
      "True Positive Rate (Sensitivity):  0.0\n",
      "False Positive Rate: 0.0\n",
      "True Negative Rate (Specificity):  1.0\n",
      "Positive Predictive Value (Precision):  0\n",
      "Negative Predictive Value:  0.993001748095\n",
      "Score (Accuracy):  0.993001748095\n"
     ]
    }
   ],
   "source": [
    "#EVALUATE ACCURACY, SENSITIVITY, SPECIFICITY...\n",
    "\n",
    "TP = 0.\n",
    "TN = 676545.\n",
    "FP = 0.\n",
    "FN = 4768.\n",
    "\n",
    "print \"BASELINE MODEL SCORE\"\n",
    "print \"True Positive Rate (Sensitivity): \",TP/(TP+FN)\n",
    "print \"False Positive Rate:\", FP/(FP+TN)\n",
    "print \"True Negative Rate (Specificity): \",TN/(TN+FP)\n",
    "print \"Positive Predictive Value (Precision): \", 0\n",
    "print \"Negative Predictive Value: \", TN/(FN+TN)\n",
    "print \"Score (Accuracy): \",(TP + TN)/(TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the baseline model is already of 99.3%. Seems like accuracy is a poor meassure fo quality of model.\n",
    "\n",
    "At click-through-rate competitions usually Submissions are evaluated using the Logarithmic Loss (smaller is better).\n",
    "https://www.kaggle.com/c/avazu-ctr-prediction/details/evaluation\n",
    "\n",
    "Let's keep this idea for later. Once we calculate average baseline model we will choose between both depending on performance scores.\n",
    "\n",
    "#####B. BASELINE PREDICTED VALUE = AVERAGE\n",
    "From now, let's calculate the predicted value in case we used average:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value for the baseline is  0.00699825190478\n"
     ]
    }
   ],
   "source": [
    "#BASELINE PREDICTED VALUE WHILE USING MOST FREQUENT:\n",
    "\n",
    "df[\"predict2\"]=df[\"0.1\"].mean() \n",
    "print \"The predicted value for the baseline is \", df[\"0.1\"].mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###LOGARITHMIC LOSS TO CALCULATE MODEL PERFORMANCE** (Log loss) **\n",
    "(source: \"Scalable Machine Learning\" MOOC)\n",
    "\n",
    "Log loss is defined as: $$  \\begin{align} \\scriptsize \\ell_{log}(p, y) = \\begin{cases} -\\log (p) & \\text{if } y = 1 \\\\\\ -\\log(1-p) & \\text{if } y = 0 \\end{cases} \\end{align} $$ \n",
    "where $ \\scriptsize p$ is a probability between 0 and 1 and $ \\scriptsize y$ is a label of either 0 or 1. \n",
    "Log loss is a standard evaluation criterion when predicting rare-events such as click-through rate prediction (it is also the criterion used in the [Criteo Kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge)).  \n",
    "\n",
    "Problem is, 0 and 1 values don't work well with log. So the trick is to use a \"epsilon\" value (very small) to avoid pure zeros as well as pure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logloss (predicted = 0, calculated by hand) = 0.17725\n",
      "\n",
      "Baseline Logloss (predicted = 0.00699825190478, calculated by hand) = 0.04170\n",
      "\n",
      "COMMENT:\n",
      "Lower performance values are better.\n",
      "So using the LogLoss function to evaluate performance, we use as baseline model\n",
      "the one that predicts an outcome equal to the average (0.06998)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FUNCTION LOGLOSS AS IMPLEMENTED BY KAGGLE:\n",
    "SOURCE: https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "import scipy as sp\n",
    "def llfun(act, pred):\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "    ll = sum(ll)\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n",
    "'''\n",
    "\n",
    "'''Implementation of logloss by 4IDIOTS:\n",
    "https://github.com/guestwalk/kaggle-avazu/tree/master/ensemble/util\n",
    "'''    \n",
    "\n",
    "#EPSILON USED: \n",
    "epsilon = 10e-12\n",
    "total = 681313.0\n",
    "\n",
    "#BASELINE CASE, PREDICTED VALUE = 0, computed by hand:\n",
    "#676545 cases in which true value = 0 and predicted = 0 -> loss += 676545*log(1)=0\n",
    "#4768 cases in which true value = 1 and predicted = 0 -> loss += 4768*log(0)\n",
    "#TOTAL of 681313\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "loss1 = 676545.0*sp.log(1.0-epsilon)+4768.0*sp.log(0.0+epsilon)\n",
    "LogLoss1 =(round(-loss1/total, 6))\n",
    "print 'Baseline Logloss (predicted = 0, calculated by hand) = {0:.5f}\\n'.format(LogLoss1)\n",
    "\n",
    "#BASELINE CASE, PREDICTED VALUE = 0.00699825190478, computed by hand:\n",
    "#676545 cases in which true value = 0 and predicted = 0.00699825190478 \n",
    "#-> loss += 676545*log(1-0.00699825190478)=0\n",
    "#4768 cases in which true value = 1 and predicted = 0.00699825190478 \n",
    "#-> loss += 4768*log(0.00699825190478)\n",
    "#TOTAL of 681313\n",
    "\n",
    "loss2 = 676545.0*sp.log(1.0-0.00699825190478)+4768.0*sp.log(0.0+0.00699825190478)\n",
    "LogLoss2 =(round(-loss2/total, 6))\n",
    "print 'Baseline Logloss (predicted = 0.00699825190478, calculated by hand) = {0:.5f}\\n'.format(LogLoss2)\n",
    "\n",
    "#Using the Logloss function, lower values are better\n",
    "print \"COMMENT:\"\n",
    "print \"Lower performance values are better.\"\n",
    "print \"So using the LogLoss function to evaluate performance, we use as baseline model\"\n",
    "print \"the one that predicts an outcome equal to the average (0.06998)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to code the logloss as a function.\n",
    "\n",
    "Function *computeTotalLogLoss(pred, true, epsylon)*\n",
    "\n",
    "Parameters:\n",
    "\n",
    "pred - predicted value\n",
    "\n",
    "true - true outcome/value\n",
    "\n",
    "epsylon - value used to avoid pure zeros/ones. By default, epsilon = 10e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logloss (predicted = 0, calculated by function) = 0.17725\n",
      "\n",
      "Baseline Logloss (predicted = 0.06998, calculated by function) = 0.04170\n",
      "\n",
      "Performance scores match the ones calculated by hand.\n"
     ]
    }
   ],
   "source": [
    "#Let's implement it in python:\n",
    "#First, the part where we calculate loss from p,y:\n",
    "import scipy as sp\n",
    "\n",
    "def computeLogLoss(p, y, epsylon=10e-12):\n",
    "    \"\"\"\n",
    "    Source: \"Scalable Machine Learning\", EdX\n",
    "    Calculates the value of log loss for a given probabilty and label.\n",
    "\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    if p == 0.0: p = epsilon\n",
    "    if p == 1.0: p = 1.0-epsilon\n",
    "    \n",
    "    if y == 1: loss = -sp.log(p)\n",
    "    if y == 0: loss = -sp.log(1.0-p)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def computeTotalLogLoss(pred,true, epsilon=10e-12):\n",
    "\n",
    "    loss, total = 0.0, 0.0\n",
    "    for i in range(0,len(pred)):\n",
    "        loss += computeLogLoss(pred[i],true[i], epsilon)\n",
    "        total += 1.0\n",
    "\n",
    "    if total == 0:\n",
    "        return('nan')\n",
    "    else:\n",
    "        return(round(loss/total, 10))\n",
    "\n",
    "#BASELINE CASE: COMPUTED USING FUNCTIONS\n",
    "#Let's compute our baseline model logarithmic loss:\n",
    "\n",
    "#a) predicted = 0\n",
    "y_true = df[\"0.1\"]\n",
    "y_pred1 = df[\"predict1\"]\n",
    "\n",
    "epsilon = 10e-12\n",
    "ModelLoss = computeTotalLogLoss(y_pred1, y_true, epsilon)\n",
    "print 'Baseline Logloss (predicted = 0, calculated by function) = {0:.5f}\\n'.format(ModelLoss)\n",
    "\n",
    "#a) predicted = 0\n",
    "y_true = df[\"0.1\"]\n",
    "y_pred2 = df[\"predict2\"]\n",
    "\n",
    "epsilon = 10e-12\n",
    "ModelLoss = computeTotalLogLoss(y_pred2, y_true, epsilon)\n",
    "print 'Baseline Logloss (predicted = 0.06998, calculated by function) = {0:.5f}\\n'.format(ModelLoss)\n",
    "\n",
    "print \"Performance scores match the ones calculated by hand.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
